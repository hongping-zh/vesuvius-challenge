"""\nDynUNet inference script for Vesuvius Challenge (AutoDL runtime).\n\n- Uses the same VesuviusDynUNet backbone as `train.py`\n- Loads config from YAML and checkpoint from disk\n- Runs sliding-window inference on a 3D volume (e.g. processed test volume)\n- Saves raw prediction probabilities to a .npy file for further post-processing / submission generation\n\nThis script is intentionally minimal and focused on correctness for AutoDL.\n"""\n\nimport argparse\nimport time\nfrom pathlib import Path\n\nimport numpy as np\nimport torch\nimport yaml\nfrom tqdm import tqdm\n\nfrom models.dynunet import VesuviusDynUNet\n\n\ndef sliding_window_inference(\n    model: torch.nn.Module,\n    volume: np.ndarray,\n    patch_size=(128, 128, 128),\n    overlap: float = 0.5,\n    batch_size: int = 2,\n    device: str = "cuda",\n):\n    """Simple 3D sliding-window inference.\n\n    Parameters\n    ----------\n    model : nn.Module\n        Trained model.\n    volume : np.ndarray\n        Input volume, shape (D, H, W).\n    patch_size : tuple\n        Patch size (pd, ph, pw).\n    overlap : float\n        Overlap ratio between patches in each dimension.\n    batch_size : int\n        Inference batch size.\n    device : str\n        Device to run inference on.\n\n    Returns\n    -------\n    np.ndarray\n        Prediction volume, shape (D, H, W), with probabilities in [0, 1].\n    """\n    model.eval()\n    model.to(device)\n\n    D, H, W = volume.shape\n    pd, ph, pw = patch_size\n\n    stride_d = max(1, int(pd * (1 - overlap)))\n    stride_h = max(1, int(ph * (1 - overlap)))\n    stride_w = max(1, int(pw * (1 - overlap)))\n\n    patches = []\n    for d in range(0, max(1, D - pd + 1), stride_d):\n        for h in range(0, max(1, H - ph + 1), stride_h):\n            for w in range(0, max(1, W - pw + 1), stride_w):\n                patches.append((d, h, w))\n\n    if not patches:\n        raise ValueError("No patches generated for given volume/patch_size.")\n\n    print(f"æ€»å…± {len(patches)} ä¸ª patches")\n\n    output = np.zeros((D, H, W), dtype=np.float32)\n    counts = np.zeros((D, H, W), dtype=np.float32)\n\n    with torch.no_grad():\n        for i in tqdm(range(0, len(patches), batch_size), desc="æ¨ç†ä¸­"):\n            batch_coords = patches[i : i + batch_size]\n            batch_data = []\n            for d, h, w in batch_coords:\n                patch = volume[d : d + pd, h : h + ph, w : w + pw]\n                # If near boundary and patch is smaller, pad it\n                pad_d = pd - patch.shape[0]\n                pad_h = ph - patch.shape[1]\n                pad_w = pw - patch.shape[2]\n                if pad_d > 0 or pad_h > 0 or pad_w > 0:\n                    patch = np.pad(\n                        patch,\n                        ((0, max(0, pad_d)), (0, max(0, pad_h)), (0, max(0, pad_w))),\n                        mode="constant",\n                    )\n                batch_data.append(patch)\n\n            batch_tensor = torch.from_numpy(np.stack(batch_data)).float().unsqueeze(1)\n            batch_tensor = batch_tensor.to(device)\n\n            preds = model(batch_tensor)\n            preds = torch.sigmoid(preds)\n            preds = preds.cpu().numpy()\n\n            for j, (d, h, w) in enumerate(batch_coords):\n                d_end = min(d + pd, D)\n                h_end = min(h + ph, H)\n                w_end = min(w + pw, W)\n                pd_eff = d_end - d\n                ph_eff = h_end - h\n                pw_eff = w_end - w\n\n                output[d:d_end, h:h_end, w:w_end] += preds[j, 0, :pd_eff, :ph_eff, :pw_eff]\n                counts[d:d_end, h:h_end, w:w_end] += 1\n\n    output = output / (counts + 1e-8)\n    return output\n\n\ndef load_config(config_path: str) -> dict:\n    config_path = Path(config_path)\n    if not config_path.is_file():\n        raise FileNotFoundError(f"Config file not found: {config_path}")\n    with open(config_path, "r", encoding="utf-8") as f:\n        return yaml.safe_load(f)\n\n\ndef build_model_from_config(config: dict, device: torch.device) -> torch.nn.Module:\n    model_cfg = config["model"]\n    if model_cfg.get("type", "dynunet") != "dynunet":\n        raise ValueError(\n            f"inference_dynunet.py expects model.type='dynunet', got {model_cfg.get('type')}"\n        )\n\n    in_channels = model_cfg["in_channels"]\n    base_num_features = model_cfg.get("base_num_features", 64)\n    out_channels = model_cfg["out_channels"]\n    deep_supervision = model_cfg.get("deep_supervision", True)\n\n    model = VesuviusDynUNet(\n        in_channels=in_channels,\n        base_num_features=base_num_features,\n        num_classes=out_channels,\n        deep_supervision=deep_supervision,\n    )\n    model.to(device)\n    return model\n\n\ndef load_checkpoint(model: torch.nn.Module, ckpt_path: str, device: torch.device) -> None:\n    ckpt_path = Path(ckpt_path)\n    if not ckpt_path.is_file():\n        raise FileNotFoundError(f"Checkpoint not found: {ckpt_path}")\n\n    print(f"ğŸ“¥ åŠ è½½æƒé‡: {ckpt_path}")\n    checkpoint = torch.load(ckpt_path, map_location=device)\n\n    # Support various checkpoint formats\n    if isinstance(checkpoint, dict):\n        if "model_state_dict" in checkpoint:\n            state_dict = checkpoint["model_state_dict"]\n        elif "state_dict" in checkpoint:\n            state_dict = checkpoint["state_dict"]\n        else:\n            state_dict = checkpoint\n    else:\n        state_dict = checkpoint\n\n    missing, unexpected = model.load_state_dict(state_dict, strict=False)\n    if missing:\n        print(f"âš ï¸  ç¼ºå°‘ {len(missing)} ä¸ªæƒé‡é”®ï¼ˆå·²å¿½ç•¥ strict=Falseï¼‰")\n    if unexpected:\n        print(f"âš ï¸  å‘ç° {len(unexpected)} ä¸ªæœªä½¿ç”¨çš„æƒé‡é”®ï¼ˆå·²å¿½ç•¥ strict=Falseï¼‰")\n\n\ndef load_volume(volume_path: str) -> np.ndarray:\n    volume_path = Path(volume_path)\n    if not volume_path.is_file():\n        raise FileNotFoundError(f"Volume file not found: {volume_path}")\n\n    print(f"ğŸ“¥ åŠ è½½ä½“æ•°æ®: {volume_path}")\n    if volume_path.suffix == ".zarr":\n        import zarr\n\n        vol = zarr.open(str(volume_path), mode="r")\n        vol = np.array(vol)\n    else:\n        vol = np.load(str(volume_path))\n\n    vol = vol.astype(np.float32)\n    print(f"  å½¢çŠ¶: {vol.shape}")\n    print(f"  èŒƒå›´: [{vol.min():.4f}, {vol.max():.4f}]")\n\n    # Simple normalization (z-score)\n    mean = vol.mean()\n    std = vol.std()\n    vol = (vol - mean) / (std + 1e-8)\n\n    return vol\n\n\ndef parse_args() -> argparse.Namespace:\n    parser = argparse.ArgumentParser(description="DynUNet inference for Vesuvius Challenge")\n    parser.add_argument(\n        "--config",\n        type=str,\n        required=True,\n        help="Path to YAML config (same as used for training)",\n    )\n    parser.add_argument(\n        "--checkpoint",\n        type=str,\n        required=True,\n        help="Path to checkpoint .pth file (e.g. best_model.pth)",\n    )\n    parser.add_argument(\n        "--volume_path",\n        type=str,\n        default="data/processed/test/volume.npy",\n        help="Path to 3D volume (.npy or .zarr) for inference",\n    )\n    parser.add_argument(\n        "--output_npy",\n        type=str,\n        default="predictions_dynunet.npy",\n        help="Output .npy file to save prediction probabilities",\n    )\n    parser.add_argument(\n        "--overlap",\n        type=float,\n        default=0.5,\n        help="Overlap ratio between patches in sliding-window inference",\n    )\n    parser.add_argument(\n        "--batch_size",\n        type=int,\n        default=2,\n        help="Inference batch size",\n    )\n    return parser.parse_args()\n\n\ndef main() -> None:\n    args = parse_args()\n\n    print("=" * 60)\n    print("DynUNet Inference (Vesuvius Challenge)")\n    print("=" * 60)\n    print()\n\n    start_time = time.time()\n\n    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")\n    print(f"ä½¿ç”¨è®¾å¤‡: {device}")\n\n    # 1. Load config and build model\n    config = load_config(args.config)\n    data_cfg = config.get("data", {})\n\n    patch_size = tuple(data_cfg.get("patch_size", [128, 128, 128]))\n    print(f"Patch size: {patch_size}")\n\n    model = build_model_from_config(config, device)\n\n    # 2. Load checkpoint\n    load_checkpoint(model, args.checkpoint, device)\n\n    # 3. Load volume\n    volume = load_volume(args.volume_path)\n\n    # 4. Sliding-window inference\n    preds = sliding_window_inference(\n        model=model,\n        volume=volume,\n        patch_size=patch_size,\n        overlap=args.overlap,\n        batch_size=args.batch_size,\n        device=str(device),\n    )\n\n    print(f"\næ¨ç†å®Œæˆï¼Œé¢„æµ‹èŒƒå›´: [{preds.min():.4f}, {preds.max():.4f}]")\n\n    # 5. Save predictions\n    output_path = Path(args.output_npy)\n    np.save(output_path, preds.astype(np.float32))\n    print(f"ğŸ“¤ é¢„æµ‹å·²ä¿å­˜åˆ°: {output_path}")\n\n    elapsed = time.time() - start_time\n    h = int(elapsed // 3600)\n    m = int((elapsed % 3600) // 60)\n    print("\n" + "=" * 60)\n    print("âœ… æ¨ç†å®Œæˆï¼")\n    print("=" * 60)\n    print(f"æ€»è€—æ—¶: {h}h {m}m")\n\n\nif __name__ == "__main__":\n    main()\n
