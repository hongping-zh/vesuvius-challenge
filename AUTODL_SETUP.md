# AutoDL 486æœº RTX 5090 é…ç½®æŒ‡å—

**ç§Ÿç”¨é…ç½®**: âœ… å·²ç¡®è®¤

---

## ğŸ–¥ï¸ æ‚¨çš„é…ç½®

```
ä¸»æœº: 486æœº
GPU: RTX 5090 (32GB) - 1/8 å¡
CPU: 25æ ¸ Xeon(R) Platinum 8470Q
å†…å­˜: 90GB
æ•°æ®ç›˜: 50GB (å¯æ‰©å®¹è‡³ 5708GB)
é©±åŠ¨: 580.76.05
CUDA: 13.0

è´¹ç”¨: ï¿¥3.03/æ—¶
```

### ğŸ’° æˆæœ¬åˆ†æ

**ä¼˜åŠ¿**:
- âœ… RTX 5090 32GBï¼ˆæ¯”è®¡åˆ’çš„ 24GB å¤š 8GBï¼ï¼‰
- âœ… 90GB å†…å­˜ï¼ˆæ¯”è®¡åˆ’çš„ 64GB å¤š 26GBï¼ï¼‰
- âœ… 25æ ¸ CPUï¼ˆæ¯”è®¡åˆ’çš„ 16æ ¸å¤š 9æ ¸ï¼ï¼‰
- âœ… è´¹ç”¨ 3.03å…ƒ/æ—¶ï¼ˆä»…æ¯”é¢„ç®—é«˜ 0.53å…ƒ/æ—¶ï¼‰

**æ€§ä»·æ¯”**: â­â­â­â­â­ æé«˜ï¼

**é¢„ç®—æ›´æ–°**:
- å¿«é€ŸéªŒè¯: 2å°æ—¶ Ã— 3.03å…ƒ = 6.06å…ƒ
- åŸºçº¿è®­ç»ƒ: 20å°æ—¶ Ã— 3.03å…ƒ = 60.6å…ƒ
- ä¼˜åŒ–è®­ç»ƒ: 30å°æ—¶ Ã— 3.03å…ƒ = 90.9å…ƒ
- æ¨ç†æµ‹è¯•: 5å°æ—¶ Ã— 3.03å…ƒ = 15.15å…ƒ

**æ€»è®¡**: ~173å…ƒ/æœˆï¼ˆæ¯”é¢„ç®—å¤š 23å…ƒï¼Œä½†é…ç½®æ›´å¥½ï¼ï¼‰

---

## ğŸš€ ç«‹å³å¼€å§‹é…ç½®

### Step 1: SSH ç™»å½•

```bash
# AutoDL ä¼šæä¾› SSH å‘½ä»¤ï¼Œç±»ä¼¼ï¼š
ssh -p [ç«¯å£] root@[IPåœ°å€]

# ä¾‹å¦‚ï¼š
ssh -p 12345 root@region-1.autodl.com
```

### Step 2: æ£€æŸ¥ç¯å¢ƒ

```bash
# æ£€æŸ¥ GPU
nvidia-smi

# åº”è¯¥çœ‹åˆ°ï¼š
# RTX 5090 32GB
# CUDA 13.0
# Driver 580.76.05

# æ£€æŸ¥ CUDA
nvcc --version

# æ£€æŸ¥ Python
python --version
```

### Step 3: å…‹éš†ä»£ç 

```bash
# åˆ›å»ºå·¥ä½œç›®å½•
mkdir -p /root/projects
cd /root/projects

# å…‹éš†ä»£ç ï¼ˆå¦‚æœå·²ä¸Šä¼ åˆ° GitHubï¼‰
git clone https://github.com/YOUR_USERNAME/vesuvius-challenge.git
cd vesuvius-challenge

# æˆ–è€…ä»æœ¬åœ°ä¸Šä¼ 
# ä½¿ç”¨ scp æˆ– AutoDL çš„æ–‡ä»¶ä¸Šä¼ åŠŸèƒ½
```

### Step 4: åˆ›å»º Conda ç¯å¢ƒ

```bash
# åˆ›å»ºç¯å¢ƒ
conda create -n vesuvius python=3.10 -y
conda activate vesuvius

# å®‰è£… PyTorch (CUDA 13.0)
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121

# å®‰è£…å…¶ä»–ä¾èµ–
pip install -r requirements.txt
```

### Step 5: é…ç½® Kaggle API

```bash
# åˆ›å»º .kaggle ç›®å½•
mkdir -p ~/.kaggle

# ä¸Šä¼  kaggle.json
# æ–¹æ³•1: ä½¿ç”¨ AutoDL æ–‡ä»¶ä¸Šä¼ åŠŸèƒ½
# æ–¹æ³•2: ä½¿ç”¨ scp
# scp kaggle.json root@[IP]:/root/.kaggle/

# è®¾ç½®æƒé™
chmod 600 ~/.kaggle/kaggle.json

# æµ‹è¯•
kaggle competitions list
```

---

## ğŸ“¥ æ•°æ®ä¸‹è½½ç­–ç•¥

### æ–¹æ¡ˆ A: ç›´æ¥ä¸‹è½½åˆ° AutoDLï¼ˆæ¨èï¼‰

```bash
# ä¸‹è½½æ•°æ®
python download_data.py

# æ•°æ®ä¼šä¿å­˜åˆ° data/raw/
```

**ä¼˜åŠ¿**: 
- ç®€å•ç›´æ¥
- æ•°æ®åœ¨è®­ç»ƒæœºå™¨ä¸Š

**æ³¨æ„**: 
- æ•°æ®ç›˜åªæœ‰ 50GB
- å¦‚æœæ•°æ®è¶…è¿‡ 50GBï¼Œéœ€è¦æ‰©å®¹

### æ–¹æ¡ˆ B: Kaggle Notebook é¢„å¤„ç†ï¼ˆçœé’±ï¼‰

**åœ¨ Kaggle Notebook è¿è¡Œ**:

```python
# 1. åŠ è½½æ•°æ®
import zarr
import numpy as np

volume = zarr.open('/kaggle/input/vesuvius-challenge-surface-detection/train/volume.zarr')

# 2. é¢„å¤„ç†
volume_norm = (volume - volume.mean()) / volume.std()

# 3. ä¿å­˜ä¸º npy
np.save('volume_processed.npy', volume_norm)

# 4. ä¸‹è½½åˆ°æœ¬åœ°ï¼Œç„¶åä¸Šä¼ åˆ° AutoDL
```

**ä¼˜åŠ¿**:
- èŠ‚çœ AutoDL è´¹ç”¨
- åˆ©ç”¨ Kaggle å…è´¹ GPU

---

## ğŸ¯ ä¼˜åŒ–é…ç½®å»ºè®®

### åˆ©ç”¨ 32GB æ˜¾å­˜ä¼˜åŠ¿

**åŸé…ç½®** (24GB):
```yaml
training:
  batch_size: 2
  patch_size: [64, 64, 64]
```

**æ–°é…ç½®** (32GB) - æ¨èï¼š
```yaml
training:
  batch_size: 3  # ä» 2 å¢åŠ åˆ° 3
  patch_size: [80, 80, 80]  # ä» 64 å¢åŠ åˆ° 80
  accumulation_steps: 3  # ç­‰æ•ˆ batch_size = 9
```

**ä¼˜åŠ¿**:
- âœ… æ›´å¤§çš„ batch size â†’ è®­ç»ƒæ›´ç¨³å®š
- âœ… æ›´å¤§çš„ patch size â†’ æ›´å¤šä¸Šä¸‹æ–‡ä¿¡æ¯
- âœ… å¯èƒ½æå‡æ¨¡å‹æ€§èƒ½

### åˆ©ç”¨ 90GB å†…å­˜ä¼˜åŠ¿

```yaml
training:
  num_workers: 8  # ä» 4 å¢åŠ åˆ° 8
  prefetch_factor: 4  # é¢„åŠ è½½æ›´å¤šæ•°æ®
```

**ä¼˜åŠ¿**:
- âœ… æ›´å¿«çš„æ•°æ®åŠ è½½
- âœ… å‡å°‘ GPU ç­‰å¾…æ—¶é—´
- âœ… æå‡è®­ç»ƒæ•ˆç‡

### åˆ©ç”¨ 25æ ¸ CPU ä¼˜åŠ¿

```python
# æ•°æ®é¢„å¤„ç†å¯ä»¥å¹¶è¡Œ
import multiprocessing as mp

# ä½¿ç”¨ 20 ä¸ªè¿›ç¨‹ï¼ˆç•™ 5 ä¸ªç»™ç³»ç»Ÿï¼‰
pool = mp.Pool(processes=20)
```

---

## ğŸ“ ä¼˜åŒ–åçš„é…ç½®æ–‡ä»¶

åˆ›å»ºæ–°é…ç½®: `configs/autodl_486.yaml`

```yaml
# AutoDL 486æœº RTX 5090 ä¼˜åŒ–é…ç½®

model:
  type: 'unet3d'  # ä½¿ç”¨æ ‡å‡†ç‰ˆï¼ˆæ˜¾å­˜å……è¶³ï¼‰
  in_channels: 1
  out_channels: 1
  base_channels: 48  # ä» 32 å¢åŠ åˆ° 48

data:
  train_dir: 'data/processed/train'
  val_dir: 'data/processed/val'
  patch_size: [80, 80, 80]  # å¢å¤§ patch size
  
training:
  batch_size: 3  # åˆ©ç”¨ 32GB æ˜¾å­˜
  accumulation_steps: 3  # ç­‰æ•ˆ batch_size = 9
  epochs: 50
  learning_rate: 0.0001
  weight_decay: 0.00001
  num_workers: 8  # åˆ©ç”¨ 25æ ¸ CPU
  prefetch_factor: 4
  save_frequency: 5
  checkpoint_dir: 'models/checkpoints'

loss:
  type: 'dice_bce'
  dice_weight: 0.5
  bce_weight: 0.5

optimizer:
  type: 'adamw'
  betas: [0.9, 0.999]
  eps: 0.00000001

scheduler:
  type: 'cosine'
  T_max: 50

augmentation:
  random_flip: true
  random_rotation: 15
  elastic_deformation: true
  intensity_shift: 0.1

logging:
  use_wandb: true
  project: 'vesuvius-challenge'
  log_frequency: 10

inference:
  patch_size: [80, 80, 80]
  overlap: 0.5
  tta: true
  batch_size: 6  # æ¨ç†æ—¶å¯ä»¥æ›´å¤§
```

---

## ğŸ”§ å¯åŠ¨è„šæœ¬

åˆ›å»º `start_training.sh`:

```bash
#!/bin/bash

echo "=========================================="
echo "Vesuvius Challenge - AutoDL 486æœº"
echo "=========================================="
echo ""

# æ¿€æ´»ç¯å¢ƒ
source activate vesuvius

# æ£€æŸ¥ GPU
echo "æ£€æŸ¥ GPU..."
nvidia-smi

echo ""
echo "å¼€å§‹è®­ç»ƒ..."
echo ""

# è®­ç»ƒ
python train.py --config configs/autodl_486.yaml

echo ""
echo "=========================================="
echo "è®­ç»ƒå®Œæˆï¼"
echo "=========================================="
```

ä½¿ç”¨ï¼š
```bash
chmod +x start_training.sh
./start_training.sh
```

---

## ğŸ“Š é¢„æœŸæ€§èƒ½

### è®­ç»ƒé€Ÿåº¦ä¼°ç®—

**é…ç½®å¯¹æ¯”**:

| é…ç½® | Batch Size | Patch Size | é€Ÿåº¦ (it/s) | Epoch æ—¶é—´ |
|------|-----------|-----------|-------------|-----------|
| åŸè®¡åˆ’ (24GB) | 2 | 64Â³ | ~0.5 | 40åˆ†é’Ÿ |
| **486æœº (32GB)** | **3** | **80Â³** | **~0.4** | **50åˆ†é’Ÿ** |

**æ€»è®­ç»ƒæ—¶é—´**:
- 50 epochs Ã— 50åˆ†é’Ÿ = ~42å°æ—¶
- è´¹ç”¨: 42å°æ—¶ Ã— 3.03å…ƒ = **127å…ƒ**

**ä¼˜åŠ¿**:
- âœ… æ›´å¤§çš„æ¨¡å‹å®¹é‡
- âœ… æ›´å¥½çš„æ€§èƒ½
- âœ… å¯èƒ½æ›´é«˜çš„åˆ†æ•°

---

## ğŸ’¾ æ•°æ®ç›˜ç®¡ç†

### æ£€æŸ¥ç©ºé—´

```bash
# æŸ¥çœ‹ç£ç›˜ä½¿ç”¨
df -h

# æŸ¥çœ‹æ•°æ®ç›®å½•å¤§å°
du -sh data/
```

### æ‰©å®¹å»ºè®®

**å¦‚æœæ•°æ®è¶…è¿‡ 50GB**:

1. åœ¨ AutoDL æ§åˆ¶å°æ‰©å®¹æ•°æ®ç›˜
2. å»ºè®®æ‰©å®¹åˆ° 200-300GB
3. è´¹ç”¨: æŒ‰éœ€è®¡è´¹

**ä¼˜åŒ–å­˜å‚¨**:
```bash
# åˆ é™¤åŸå§‹æ•°æ®ï¼Œåªä¿ç•™é¢„å¤„ç†åçš„
rm -rf data/raw/*.zip

# å‹ç¼©æ£€æŸ¥ç‚¹
tar -czf checkpoints.tar.gz models/checkpoints/
```

---

## ğŸ¯ è®­ç»ƒç›‘æ§

### WandB é…ç½®

```bash
# ç™»å½• WandB
wandb login

# è¾“å…¥ API Keyï¼ˆä» https://wandb.ai/settings è·å–ï¼‰
```

### å®æ—¶ç›‘æ§

```bash
# æ–¹æ³•1: WandB ä»ªè¡¨æ¿
https://wandb.ai/your-username/vesuvius-challenge

# æ–¹æ³•2: TensorBoard
tensorboard --logdir logs/ --port 6006

# æ–¹æ³•3: æŸ¥çœ‹æ—¥å¿—
tail -f logs/training.log
```

### GPU ç›‘æ§

```bash
# å®æ—¶ç›‘æ§ GPU
watch -n 1 nvidia-smi

# æˆ–ä½¿ç”¨ gpustat
pip install gpustat
gpustat -i 1
```

---

## âš¡ å¿«é€Ÿå‘½ä»¤å‚è€ƒ

```bash
# æ¿€æ´»ç¯å¢ƒ
conda activate vesuvius

# å¼€å§‹è®­ç»ƒ
python train.py --config configs/autodl_486.yaml

# æ¢å¤è®­ç»ƒ
python train.py --resume models/checkpoints/checkpoint_epoch_20.pth

# æ¨ç†
python inference.py --checkpoint models/checkpoints/best_model.pth

# æŸ¥çœ‹ GPU
nvidia-smi

# æŸ¥çœ‹è¿›ç¨‹
ps aux | grep python

# æ€æ­»è¿›ç¨‹
kill -9 [PID]
```

---

## ğŸŠ æ€»ç»“

### æ‚¨çš„é…ç½®ä¼˜åŠ¿

âœ… **RTX 5090 32GB** - æ¯”è®¡åˆ’å¤š 8GB  
âœ… **90GB å†…å­˜** - æ¯”è®¡åˆ’å¤š 26GB  
âœ… **25æ ¸ CPU** - æ¯”è®¡åˆ’å¤š 9æ ¸  
âœ… **è´¹ç”¨ 3.03å…ƒ/æ—¶** - ä»…æ¯”é¢„ç®—é«˜ 0.53å…ƒ/æ—¶  

### ä¼˜åŒ–å»ºè®®

1. **ä½¿ç”¨æ›´å¤§çš„ batch size** (3 vs 2)
2. **ä½¿ç”¨æ›´å¤§çš„ patch size** (80Â³ vs 64Â³)
3. **ä½¿ç”¨æ ‡å‡† U-Net** (ä¸éœ€è¦è½»é‡ç‰ˆ)
4. **å¢åŠ  num_workers** (8 vs 4)

### é¢„æœŸæˆæœ

- **è®­ç»ƒæ—¶é—´**: ~42å°æ—¶
- **æ€»è´¹ç”¨**: ~127å…ƒ
- **æ€§èƒ½**: æ¯”åŸºçº¿é…ç½®æ›´å¥½
- **ç›®æ ‡**: Top 10%

---

**é…ç½®å®Œç¾ï¼ç«‹å³å¼€å§‹å§ï¼** ğŸš€

**æœ‰ä»»ä½•é—®é¢˜éšæ—¶æ‰¾æˆ‘ï¼** ğŸ’ª
